# How the System Knows Which Groq Model to Call

## ğŸ“‹ Quick Answer

The system determines which Groq model to call through this priority chain:

```
1. Explicit parameter (if provided) â†’ highest priority
2. Environment variable: GROQ_SELECTED_MODEL
3. Environment variable: DEFAULT_MODEL
4. Fallback: raises RuntimeError "No model selected"
```

---

## ğŸ” Detailed Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Action                               â”‚
â”‚  â€¢ Starts chat OR                                            â”‚
â”‚  â€¢ Sends message                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Chat Route Handler                              â”‚
â”‚  POST /api/chat/start                                        â”‚
â”‚  POST /api/chat/message                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ConversationService.start_investigation()          â”‚
â”‚  OR ConversationService.process_answer()                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               LLMService.initialize_provider()               â”‚
â”‚                                                              â”‚
â”‚  Step 1: Get provider                                        â”‚
â”‚    if provider is None:                                      â”‚
â”‚      provider = config.get_active_provider()                 â”‚
â”‚      # Reads: ACTIVE_PROVIDER env var                        â”‚
â”‚                                                              â”‚
â”‚  Step 2: Get model_id                                        â”‚
â”‚    if model_id is None:                                      â”‚
â”‚      model_id = config.get_selected_model()  â†â”€â”€ HERE!       â”‚
â”‚      # This is where model is determined                     â”‚
â”‚                                                              â”‚
â”‚  Step 3: Get API key                                         â”‚
â”‚    api_key = config.get_token(provider)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ConfigService.get_selected_model(provider)           â”‚
â”‚                                                              â”‚
â”‚  def get_selected_model(provider=None):                      â”‚
â”‚    if not provider:                                          â”‚
â”‚      provider = get_active_provider()                        â”‚
â”‚      # Returns: ACTIVE_PROVIDER or "groq"                    â”‚
â”‚                                                              â”‚
â”‚    # For Groq, this becomes:                                 â”‚
â”‚    env_var = "GROQ_SELECTED_MODEL"                           â”‚
â”‚    model = os.getenv("GROQ_SELECTED_MODEL")                  â”‚
â”‚                                                              â”‚
â”‚    # Returns model from .env file                            â”‚
â”‚    return model  # e.g., "llama2-70b-4096"                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       ModelChecker.get_langchain_model()                     â”‚
â”‚                                                              â”‚
â”‚  Creates the actual LangChain model instance:                â”‚
â”‚                                                              â”‚
â”‚  if provider == "groq":                                      â”‚
â”‚    return ChatGroq(                                          â”‚
â”‚      groq_api_key=api_key,                                   â”‚
â”‚      model_name=model_id,  â†â”€â”€ "llama2-70b-4096"             â”‚
â”‚      temperature=0.7,                                        â”‚
â”‚      max_tokens=2000                                         â”‚
â”‚    )                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Groq API Call                               â”‚
â”‚  POST https://api.groq.com/openai/v1/chat/completions       â”‚
â”‚  {                                                           â”‚
â”‚    "model": "llama2-70b-4096",  â†â”€â”€ Model is specified here  â”‚
â”‚    "messages": [...],                                        â”‚
â”‚    "temperature": 0.7,                                       â”‚
â”‚    "max_tokens": 2000                                        â”‚
â”‚  }                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Configuration Storage

The model selection is stored in **two places**:

### 1. Environment Variable (`.env` file)
```bash
# backend/.env
GROQ_SELECTED_MODEL='llama2-70b-4096'
```

### 2. Memory (Runtime)
```python
# In LLMService instance
self.model_id = "llama2-70b-4096"
```

---

## ğŸ”„ How Model Gets Set

### Method 1: Via Web UI (Most Common)

```
User Action Flow:
1. User goes to Settings
2. Clicks "Fetch Models" button
   â†“
3. Frontend calls: GET /api/config/models?provider=groq
   â†“
4. Backend fetches available models from Groq API
   â†“
5. Frontend displays list (e.g., llama2-70b-4096, mixtral-8x7b, etc.)
   â†“
6. User selects "llama-3.3-70b-versatile"
   â†“
7. Frontend calls: POST /api/config/model/select
   {
     "provider": "groq",
     "model_id": "llama-3.3-70b-versatile"
   }
   â†“
8. Backend (ConfigService.save_selected_model):
   - Writes to .env file: GROQ_SELECTED_MODEL=llama-3.3-70b-versatile
   - Sets ACTIVE_PROVIDER=groq
   â†“
9. Done! Model is now configured
```

### Method 2: Via Environment Variable

```bash
# Edit backend/.env
GROQ_SELECTED_MODEL=llama-3.3-70b-versatile
ACTIVE_PROVIDER=groq
```

### Method 3: Via Setup Script

```bash
./setup_groq_key.sh
# Script automatically sets:
# GROQ_SELECTED_MODEL=llama-3.3-70b-versatile
```

---

## ğŸ” Code Trace Example

Let's trace a real example when a user sends a chat message:

### Step 1: User sends message
```javascript
// Frontend
POST /api/chat/message
{
  "session_id": "abc123",
  "message": "I want to build a task manager"
}
```

### Step 2: Chat route receives request
```python
# backend/routes/chat_routes.py
@router.post("/api/chat/message")
async def send_message(request: MessageRequest):
    conv_service = ConversationService()
    result = await conv_service.process_answer(
        session_id=request.session_id,
        answer=request.message
    )
```

### Step 3: Conversation service needs LLM
```python
# backend/services/conversation_service.py
async def process_answer(self, session_id, answer):
    # Need to generate next question using LLM
    llm_service = LLMService(config_service, model_checker)
    
    # Initialize LLM (no explicit model_id passed)
    llm_service.initialize_provider()  # â† No parameters!
```

### Step 4: LLM service determines model
```python
# backend/services/llm_service.py
def initialize_provider(self, provider=None, model_id=None):
    # provider is None, so get from config
    if provider is None:
        provider = self.config.get_active_provider()
        # Returns: "groq" (from ACTIVE_PROVIDER in .env)
    
    # model_id is None, so get from config
    if model_id is None:
        model_id = self.config.get_selected_model()
        # â† THIS IS WHERE IT HAPPENS!
```

### Step 5: Config service reads environment
```python
# backend/services/config_service.py
def get_selected_model(self, provider=None):
    if not provider:
        provider = self.get_active_provider()
        # Returns: "groq"
    
    # Build environment variable name
    env_var = f"{provider.upper()}_SELECTED_MODEL"
    # env_var = "GROQ_SELECTED_MODEL"
    
    # Read from environment
    model = os.getenv("GROQ_SELECTED_MODEL")
    # Returns: "llama2-70b-4096"
    
    return model
```

### Step 6: Model checker creates instance
```python
# backend/services/model_checker.py
def get_langchain_model(self, provider, model_id, api_key, ...):
    if provider == "groq":
        return ChatGroq(
            groq_api_key=api_key,
            model_name=model_id,  # â† "llama2-70b-4096"
            temperature=temperature,
            max_tokens=max_tokens
        )
```

### Step 7: LangChain makes API call
```python
# LangChain internally
response = await groq_client.chat.completions.create(
    model="llama2-70b-4096",  # â† Sent to Groq API
    messages=[...],
    temperature=0.7
)
```

---

## ğŸ¯ Current Configuration

Based on your `.env` file:

```bash
ACTIVE_PROVIDER='groq'
GROQ_SELECTED_MODEL='llama2-70b-4096'
```

**This means:**
- Provider: Groq Cloud
- Model: llama2-70b-4096 (Llama 2 70B, 4K context window)

---

## ğŸ”§ How to Change the Model

### Option 1: Via UI
```
1. Go to Settings
2. Click "Fetch Models"
3. Select different model (e.g., llama-3.3-70b-versatile)
4. Click "Select Model"
```

### Option 2: Edit .env
```bash
# backend/.env
GROQ_SELECTED_MODEL='llama-3.3-70b-versatile'
```

### Option 3: API Call
```bash
curl -X POST http://localhost:8000/api/config/model/select \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "groq",
    "model_id": "llama-3.3-70b-versatile"
  }'
```

---

## ğŸš¨ What Happens If No Model Is Set?

```python
# In LLMService.initialize_provider()
if model_id is None:
    model_id = self.config.get_selected_model()
    if not model_id:
        raise RuntimeError("No model selected")
        # Application will fail with clear error message
```

**Error Message:**
```
RuntimeError: No model selected
```

**Solution:**
- Set `GROQ_SELECTED_MODEL` in `.env`, OR
- Select a model via Settings UI, OR
- Pass `model_id` explicitly when initializing

---

## ğŸ“Š Priority Order Summary

When determining which model to use:

| Priority | Source | Example |
|----------|--------|---------|
| 1 (Highest) | Explicit parameter | `initialize_provider(model_id="mixtral-8x7b")` |
| 2 | Environment variable | `GROQ_SELECTED_MODEL='llama2-70b-4096'` |
| 3 | Default fallback | `DEFAULT_MODEL='llama2-70b-4096'` (if set) |
| 4 (Lowest) | Error raised | `RuntimeError: No model selected` |

---

## ğŸ’¡ Best Practices

### 1. Always Set a Model
```bash
# In .env
GROQ_SELECTED_MODEL=llama-3.3-70b-versatile
```

### 2. Use UI for Easy Switching
The Settings panel makes it easy to switch between models without editing files.

### 3. Different Models for Different Tasks
```python
# Example: Use fast model for simple queries
llm_service.initialize_provider(
    provider="groq",
    model_id="llama-3.1-8b-instant"  # Fast model
)

# Use powerful model for complex reasoning
llm_service.initialize_provider(
    provider="groq",
    model_id="mixtral-8x7b-32768"  # Complex reasoning
)
```

---

## ğŸ” Debugging Model Selection

### Check Current Configuration
```bash
# See what model is configured
curl http://localhost:8000/api/config/status
```

**Response:**
```json
{
  "active_provider": "groq",
  "has_groq_token": true,
  "has_openai_token": true,
  "selected_model": "llama2-70b-4096"  â† Current model
}
```

### Check Environment Variables
```bash
cd backend
cat .env | grep SELECTED_MODEL
```

### Check Runtime Configuration
```python
# In Python console
from services.config_service import ConfigService

config = ConfigService()
provider = config.get_active_provider()
model = config.get_selected_model(provider)

print(f"Provider: {provider}")
print(f"Model: {model}")
```

---

## ğŸ“– Related Files

- **Model Selection Logic**: `backend/services/config_service.py` (lines 242-262)
- **LLM Initialization**: `backend/services/llm_service.py` (lines 59-115)
- **Model Instance Creation**: `backend/services/model_checker.py` (lines 280-340)
- **API Endpoints**: `backend/routes/config_routes.py`
- **Environment Config**: `backend/.env`

---

## âœ… Summary

**The system knows which Groq model to call by:**

1. **Reading** `GROQ_SELECTED_MODEL` from the `.env` file
2. **Using** that value when creating the `ChatGroq` instance
3. **Passing** it to the Groq API in the `model` parameter

**The model is set by:**
- User selecting it in the Settings UI, OR
- Developer editing the `.env` file, OR
- API call to `/api/config/model/select`

**Current model:** `llama2-70b-4096` (from your `.env`)

**To change:** Go to Settings â†’ Fetch Models â†’ Select a different model â†’ Done!

---

*For more details, see: `backend/services/config_service.py` and `backend/services/llm_service.py`*
